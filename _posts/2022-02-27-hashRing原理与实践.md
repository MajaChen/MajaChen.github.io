---
layout: post
title: falcon系列之——基于hashring实现负载均衡
subtitle: hashring falcon 负载均衡
cover-img: /assets/img/15.jpg
thumbnail-img: /assets/img/animals/TawnyFrogmouth.jpg
tags: [falcon 负载均衡]
---

**前言**

&emsp;&emsp;falcon是一款优秀的、成功的监控软件，由小米开源，可访问[项目主页](http://open-falcon.org/)获取详情。

&emsp;&emsp;我有幸在小米falcon项目组从事开发工作，通过阅读falcon源代码，我发现falcon在技术层面解决的主要问题是：数据密集型场景下的数据治理——数据量这么大，服务怎么不垮？查询怎么够快？

&emsp;&emsp;其中的技术点包括：

- beego
- gin
- grpc
- hashring(负载均衡)
- 池化设计
- 缓存
- 队列限长、channel限长
- 数据库(tsdb,es)访问限流
- 基础设施（多机房、多集群）

&emsp;&emsp;我将开辟专题系列来逐一剖析上述技术点，从而积累构建数据密集型应用的经验。

# hash环在falcon系统中的应用

一致性hash算法的相关理论可参见[博客文章](https://writings.sh/post/consistent-hashing-algorithms-part-1-the-problem-and-the-concept)。

## 解决一个什么问题

&emsp;&emsp;数据分发问题，在transfer模块如何均匀地把大量指标数据分发到同个数据源的不同节点，从而实现不同节点间的数据均衡。注：不涉及到节点增减带来的数据的迁移问题，即只利用hash环写数据，不利用其读数据。

## 实施步骤

### step1：从数据库表结构说起

```sql
create table falcon.hashring
(
    id           int(4) unsigned auto_increment
        primary key,
    node_name    varchar(128) not null comment '节点名称',
    node_value   varchar(128) not null comment '节点值',
    idc          varchar(10)  not null comment '机房',
    service_name varchar(128) not null comment '服务名:graph or judge'
)
```

### step2：查询数据库

```go
hashRingData := hashring.QueryDB(cfg.Idc, serviceName)

type HashRing struct {
	NodeName  string `json:"node_name"`
	NodeValue string `json:"node_value"`
}

func QueryDB(idc, serviceName string) (hashRing []HashRing) {
	sql := fmt.Sprintf("select node_name, node_value from hashring where idc = '%s' and service_name = '%s'", idc, serviceName)
	rows, err := config.DBConn.Query(sql)
	defer rows.Close()
	if err != nil {
		log.Error("query hash node fail, error:", err)
		return hashRing
	}

	for rows.Next() {
		var nodeName, nodeValue string
		var hashNode HashRing
		err = rows.Scan(&nodeName, &nodeValue)
		if err != nil {
			log.Error("scan query result error:", err)
			continue
		}
		hashNode.NodeName = nodeName
		hashNode.NodeValue = nodeValue
		hashRing = append(hashRing, hashNode)
	}
	return
}
```

&emsp;&emsp;以idc和serviceName为索引，从数据库中查询数据，同个<idc,serviceName>组合，对应多个<nodeName,nodeValue>。

### step3：格式化数据

```go
cluster := hashring.FormatHashRingData(hashRingData)
cfg.NodeCluster = config.FormatClusterItems(cluster)
var nodeNames []string
for _, item := range hashRingData {
	nodeNames = append(nodeNames, item.NodeName)
}

func FormatHashRingData(data []HashRing) map[string]string {
   ret := make(map[string]string)
   for _, node := range data {
      ret[node.NodeName] = node.NodeValue
   }
   return ret
}

// map["node"]="host1,host2" --> map["node"]=["host1", "host2"]
func FormatClusterItems(cluster map[string]string) map[string]*ClusterNode {
   ret := make(map[string]*ClusterNode)
   for node, clusterStr := range cluster {
      items := strings.Split(clusterStr, ",")
      nitems := make([]string, 0)
      for _, item := range items {
         nitems = append(nitems, strings.TrimSpace(item))
      }
      ret[node] = NewClusterNode(nitems)
   }

   return ret
}

NodeCluster          map[string]*ClusterNode `json:"-"`
type ClusterNode struct {
	Addrs []string `json:"addrs"`
}
```

&emsp;&emsp;这一步是将<nodeName,nodeValue>数组转成map结构，nodeName为key，nodeValue为value。然后考虑到value可能包含多个地址的情况，即一个nodeName，对应多个addr（addr为物理节点地址），存入cfg.NodeCluster。然后收集所有nodeName，存入nodeNames。

### step4：关键一步，构建NodeRing

```go
NodeRing = cconsistent.NewConsistentHashNodesRing(cfg.Replicas, nodeNames)

func NewConsistentHashNodesRing(numberOfReplicas int, nodes []string) *ConsistentHashNodeRing {
	ret := &ConsistentHashNodeRing{ring: New()}
	ret.SetNumberOfReplicas(numberOfReplicas)
	ret.SetNodes(nodes)
	return ret
}

// 设置副本数，默认500
func (this *ConsistentHashNodeRing) SetNumberOfReplicas(num int) {
	this.ring.NumberOfReplicas = num
}

// 将节点逐个加入hash环，节点由nodeName代表
func (this *ConsistentHashNodeRing) SetNodes(nodes []string) {
	for _, node := range nodes {
		this.ring.Add(node)
	}
}

type ConsistentHashNodeRing struct {
	ring *Consistent
}
type Consistent struct {
	circle           map[uint32]string
	members          map[string]bool
	sortedHashes     uints
	NumberOfReplicas int
	count            int64
	scratch          [64]byte
	sync.RWMutex
}
type uints []uint32

func (c *Consistent) Add(elt string) {
	c.Lock()
	defer c.Unlock()
	c.add(elt)
}

// 对nodeName取hash值，混入i，得到500个“混合hash值”,各hash值对应相同的nodeName,存入cicle
// 即，每个nodeName在cicle中对应500个随即条目(影子节点)
// 然后将不同nodeName对应的混合hash值进行排序，存入sortedHashes
func (c *Consistent) add(elt string) {
	for i := 0; i < c.NumberOfReplicas; i++ {
		c.circle[c.hashKey(c.eltKey(elt, i))] = elt
	}
	c.members[elt] = true
	c.updateSortedHashes()
	c.count++
}

func (c *Consistent) updateSortedHashes() {
	hashes := c.sortedHashes[:0]
	//reallocate if we're holding on to too much (1/4th)
	if cap(c.sortedHashes)/(c.NumberOfReplicas*4) > len(c.circle) {
		hashes = nil
	}
	for k := range c.circle {
		hashes = append(hashes, k)
	}
	sort.Sort(hashes)
	c.sortedHashes = hashes
}

```

&emsp;&emsp;存储示意图如下：

![image-20220227153215393](https://gitee.com/xinyuanchen/image_collection/raw/master/image-20220227153215393.png)

&emsp;&emsp;我们可以从图中看到sortedHashes顺序存储hash值，hash值对应cycle map中的key。每个nodeName对应500个hash值(不考虑hash值重叠的情况)。

### step5：运行查询

&emsp;&emsp;借助nodeRing决定数据应该被存入哪个nodeName对应的节点中，即完成数据->nodeName的映射。

```go
// 提取数据特征（Endpoint，Metric，Tags）构造PK
func (t *MetaData) PK() string {
	return utils.PK(t.Endpoint, t.Metric, t.Tags)
}
pk := item.(*cmodel.MetaData).PK()
node, err := graph.NodeRing.GetNode(pk)

// 
func (this *ConsistentHashNodeRing) GetNode(pk string) (string, error) {
	return this.ring.Get(pk)
}

// 借助nodeRing，确定PK对应的nodeName
// 首先对PK计算hash值，然后确定该hash值在sortedHashes中的位置（寻找sortedHashes中第一个比他大的元素）
// 去cicle中取出该位置对应的nodenName
func (c *Consistent) Get(name string) (string, error) {
	c.RLock()
	defer c.RUnlock()
	if len(c.circle) == 0 {
		return "", ErrEmptyCircle
	}
	key := c.hashKey(name)
	i := c.search(key)
	return c.circle[c.sortedHashes[i]], nil
}

func (c *Consistent) hashKey(key string) uint32 {
	hash := murmur3.New32()
	_, _ = hash.Write([]byte(key))
	return hash.Sum32()
}

func (c *Consistent) search(key uint32) (i int) {
	f := func(x int) bool {
		return c.sortedHashes[x] > key
	}
	i = sort.Search(len(c.sortedHashes), f)
	if i >= len(c.sortedHashes) {
		i = 0
	}
	return
}
// 根据nodeName,去cfg.NodeCluster中找出对应的物理地址，将数据传送到物理地址对应的主机。
```

&emsp;&emsp;基本逻辑是：根据数据特征计算hash值，根据hash值去sortedHashes中寻找首个比它大的元素，根据该元素去cicle中寻找nodeName。

&emsp;&emsp;比如，数据的hash值经计算是9，那么search函数会返回15在sortedHashes中对应的下标，在去cycle中寻找对应的nodeName——node_name_1,然后去clusterInfo中取出node_name_1对应的节点物理地址。

![hashring](https://gitee.com/xinyuanchen/image_collection/raw/master/hashring.png)



## 原理总结

&emsp;&emsp;一方面，在0~2的32次方减1之间的“环空间”内，对每个节点，生成500个影子节点，基于hash取值均匀分布的特点，影子节点在环空间内也是均匀分布的；另一方面，计算待分发数据的hash值，确定该hash值在hash环上命中哪个影子节点，将数据发送到对应的实节点，从而达到数据均匀分布的效果。
## 效果点评
&emsp;&emsp;通过增加影子节点，hash环能够将数据均匀映射到不同物理节点（均匀性通过各节点承载的数据量之间的标准差衡量，从略）。映射操作的时间复杂度是O(lgN)，时间耗费于二分查找。

&emsp;&emsp;主要缺点是没有考虑数据迁移，所以集群物理节点数目无法变更。
